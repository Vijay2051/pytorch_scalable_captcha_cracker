{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../input/captcha_v2\"\n",
    "BATCH_SIZE = 200\n",
    "IMAGE_WIDTH = 300\n",
    "IMAGE_HEIGHT = 65\n",
    "NUM_WORKERS = 8\n",
    "EPOCHS = 10\n",
    "DEVICE = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "# ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "class ClassificationDataset:\n",
    "    def __init__(self, image_paths, targets, resize=None):\n",
    "        self.images = image_paths\n",
    "        self.targets = targets\n",
    "        self.resize = resize\n",
    "\n",
    "        self.aug = albumentations.Compose(\n",
    "            [albumentations.Normalize(\n",
    "                mean=[0.4914, 0.4822, 0.4465],\n",
    "                std=[0.2023, 0.1994, 0.2010], always_apply=True),\n",
    "             albumentations.Resize(300, 65, always_apply=True)\n",
    "             ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image = Image.open(self.images[item]).convert(\"RGB\")\n",
    "        target = self.targets[item]\n",
    "\n",
    "        # Pil accepts resize in the width first approach, so when resizing the image width should be first and height should be second\n",
    "        if self.resize is not None:\n",
    "            image = image.resize(\n",
    "                (self.resize[1], self.resize[0]), resample=Image.BILINEAR\n",
    "            )\n",
    "\n",
    "        # convert the images into numpy array before applying the augmentations\n",
    "        image = np.array(image)\n",
    "        aug_image = self.aug(image=image)\n",
    "        image = aug_image[\"image\"]\n",
    "\n",
    "        # we should transpose these numpy arrays into torch versions of transposed images\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "\n",
    "        # convert outputs into tensor\n",
    "        return {\n",
    "            \"images\": torch.tensor(image, dtype=torch.float),\n",
    "            \"targets\": torch.tensor(target, dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.functional import dropout, log_softmax\n",
    "\n",
    "\n",
    "class CaptchaModel(nn.Module):\n",
    "    def __init__(self, num_chars):\n",
    "        super(CaptchaModel, self).__init__()\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(3, 128, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.conv_2 = nn.Conv2d(128, 64, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.linear_1 = nn.Linear(1152, 64)\n",
    "        self.drop_1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            64, 32, bidirectional=True, num_layers=2, dropout=0.25, batch_first=True\n",
    "        )\n",
    "        self.output = nn.Linear(64, num_chars + 1)\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        bs, ch, ht, wd = images.size()\n",
    "        print(bs, ch, ht, wd)\n",
    "        x = F.relu(self.conv_1(images))\n",
    "        print(x.size())\n",
    "        x = self.max_pool1(x)\n",
    "        print(x.size())\n",
    "\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        print(x.size())\n",
    "        x = self.max_pool2(x)  # 1, 64, 18, 75\n",
    "        print(\n",
    "            x.size()\n",
    "        )  # before passing these outputs into custom rnn permute the outputs (0, 3, 1, 2)\n",
    "        x = x.permute(\n",
    "            0, 3, 1, 2\n",
    "        )  # 1, 75, 64, 18   # because we have to go through the width of the images\n",
    "        print(\"1st permute: \", x.size())\n",
    "        x = x.view(bs, x.size(1), -1)\n",
    "        print(x.size())\n",
    "        x = self.linear_1(x)\n",
    "        x = self.drop_1(x)\n",
    "        print(x.size())\n",
    "        x, _ = self.gru(x)\n",
    "        print(x.size())\n",
    "        x = self.output(x)\n",
    "        print(x.size())\n",
    "        # To calculate the ctc loss, we should again permute it\n",
    "        # this you have to remember, timestamps, batches, values\n",
    "        x = x.permute(1, 0, 2)\n",
    "        print(x.shape)\n",
    "\n",
    "        if targets is not None:\n",
    "            # ctc loss is already implemented in pytorch, but it is not straight forward.\n",
    "            # it takes log softmax values.\n",
    "            log_softmax_values = F.log_softmax(\n",
    "                x, 2\n",
    "            )  # (x, 2) indicates, x th second index which is num_chars + 1\n",
    "\n",
    "            # Two things have to specified here, length of inputs and len of outputs\n",
    "            input_lengths = torch.full(\n",
    "                size=(bs,), fill_value=log_softmax_values.size(0), dtype=torch.int32\n",
    "            )\n",
    "            # print(input_lengths)\n",
    "            targets_lengths = torch.full(\n",
    "                size=(bs,), fill_value=targets.size(1), dtype=torch.int32\n",
    "            )\n",
    "            # print(targets_lengths)\n",
    "            loss = nn.CTCLoss(blank=0)(\n",
    "                log_softmax_values, targets, input_lengths, targets_lengths\n",
    "            )\n",
    "\n",
    "            return x, loss\n",
    "\n",
    "        return x, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    fn_loss = 0\n",
    "    tk = tqdm(dataloader, total=len(dataloader))\n",
    "    # print(tk)\n",
    "    for data in tk:\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        _, loss = model(**data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        fn_loss += loss.item()\n",
    "    return fn_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def eval(model, dataloader):\n",
    "    model.eval()\n",
    "    fn_loss = 0\n",
    "    fn_preds = []\n",
    "    tk = tqdm(dataloader, total=len(dataloader))\n",
    "    with torch.no_grad():\n",
    "        for data in tk:\n",
    "            for k, v in data.items():\n",
    "                data[k] = v.to(DEVICE)\n",
    "            batch_preds, loss = model(**data)\n",
    "            fn_loss += loss.item()\n",
    "            fn_preds.append(batch_preds)\n",
    "    return fn_preds, fn_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[&#39;../input/raw_captcha/kineets.png&#39;, &#39;../input/raw_captcha/unrther.png&#39;, &#39;../input/raw_captcha/turning.png&#39;, &#39;../input/raw_captcha/marrts.png&#39;]\n[&#39;kineets&#39;, &#39;unrther&#39;, &#39;turning&#39;, &#39;marrts&#39;, &#39;ampand&#39;]\n2926\n35\n2340 2340\n586 586\n"
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File &quot;/home/rahul/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py&quot;, line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File &quot;/home/rahul/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 47, in fetch\n    return self.collate_fn(data)\n  File &quot;/home/rahul/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&quot;, line 74, in default_collate\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File &quot;/home/rahul/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&quot;, line 74, in &lt;dictcomp&gt;\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File &quot;/home/rahul/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&quot;, line 55, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [7] at entry 0 and [6] at entry 1\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m&lt;ipython-input-16-3aef83f6648a&gt;\u001b[0m in \u001b[0;36m&lt;module&gt;\u001b[0;34m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m&quot;__main__&quot;\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---&gt; 94\u001b[0;31m     \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m&lt;ipython-input-16-3aef83f6648a&gt;\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---&gt; 78\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--&gt; 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--&gt; 989\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-&gt; 1014\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--&gt; 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File &quot;/home/rahul/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py&quot;, line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File &quot;/home/rahul/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 47, in fetch\n    return self.collate_fn(data)\n  File &quot;/home/rahul/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&quot;, line 74, in default_collate\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File &quot;/home/rahul/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&quot;, line 74, in &lt;dictcomp&gt;\n    return {key: default_collate([d[key] for d in batch]) for key in elem}\n  File &quot;/home/rahul/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&quot;, line 55, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [7] at entry 0 and [6] at entry 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from numpy.lib.shape_base import split\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing, model_selection\n",
    "import glob\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def split(x):\n",
    "    return [i for i in str(x)]\n",
    "\n",
    "\n",
    "def run_training():\n",
    "    # image_files\n",
    "    image_files = glob.glob(\"../input/raw_captcha/*.png\")\n",
    "    print(image_files[:4])\n",
    "\n",
    "    # targets\n",
    "    targets_orig = [i.split(\"/\")[-1][:-4] for i in image_files]\n",
    "    print(targets_orig[:5])\n",
    "\n",
    "    # creating a list of list for the targets\n",
    "    targets = [[j for j in i] for i in targets_orig]\n",
    "\n",
    "    # flattening the lists\n",
    "    targets_flat = [item for sublists in targets for item in sublists]\n",
    "    # print(targets_flat)\n",
    "\n",
    "    lbl_encoder = preprocessing.LabelEncoder()\n",
    "    lbl_encoder.fit(targets_flat)\n",
    "    enc_targets = [lbl_encoder.transform(x) for x in targets]\n",
    "\n",
    "    # this +1 is to add 1 to all the encoded labels, so that we could use 0 for the unknown values\n",
    "    enc_targets = np.array(enc_targets) + 1\n",
    "    print(len(enc_targets))\n",
    "    print(len(lbl_encoder.classes_))\n",
    "\n",
    "    (\n",
    "        train_imgs,\n",
    "        test_imgs,\n",
    "        train_targets_orig,\n",
    "        test_target_orig,\n",
    "        train_targets,\n",
    "        test_targets,\n",
    "    ) = model_selection.train_test_split(\n",
    "        image_files, targets_orig, enc_targets, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    print(len(train_imgs), len(train_targets))\n",
    "    print(len(test_imgs), len(test_targets))\n",
    "    train_dataset = ClassificationDataset(\n",
    "        image_paths=train_imgs,\n",
    "        targets=train_targets,\n",
    "        resize=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    )\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    test_dataset = ClassificationDataset(\n",
    "        image_paths=test_imgs,\n",
    "        targets=test_targets,\n",
    "        resize=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    )\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    for data in train_dataloader:\n",
    "        print(data)\n",
    "\n",
    "    model = CaptchaModel(num_chars=len(lbl_encoder.classes_))\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, factor=0.8, patience=5, verbose=True\n",
    "    )\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train(model, train_dataloader, optimizer)\n",
    "        valid_preds, valid_loss = eval(model, test_dataloader)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}